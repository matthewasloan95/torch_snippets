{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp s3_loader2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import boto3\n",
    "import os\n",
    "import datetime\n",
    "from datetime import tzinfo\n",
    "from dateutil.tz import tzutc\n",
    "from torch_snippets import stem, fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_snippets.s3_loader2 import S3FileHandler\n",
    "\n",
    "aws_access_key_id = \"AKIAQFXXXXXXXX6CN\"\n",
    "aws_secret_access_key = \"AC3XXXXZXXXXXXXXXXXXXXXXXXejfXXXXXXh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# | hide\n",
    "class S3FileHandler:\n",
    "    def __init__(self, aws_access_key, aws_secret_access_key):\n",
    "        self.s3_client = boto3.client(\n",
    "            \"s3\",\n",
    "            aws_access_key_id=aws_access_key_id,\n",
    "            aws_secret_access_key=aws_secret_access_key,\n",
    "        )\n",
    "\n",
    "    def list_s3_buckets(self):\n",
    "        \"\"\"\n",
    "        Lists all the s3 buckets in s3\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Call S3 to list current buckets\n",
    "            response = self.s3_client.list_buckets()\n",
    "            buckets = [bucket[\"Name\"] for bucket in response[\"Buckets\"]]\n",
    "            return buckets\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    def list_s3_objects(self, bucket_name, key=\"\"):\n",
    "        \"\"\"\n",
    "        List all files in an S3 bucket or within a specific prefix.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param key: str or None. Specific prefix to list files from, defaults to None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize a paginator for listing objects\n",
    "            paginator = self.s3_client.get_paginator(\"list_objects_v2\")\n",
    "            # Use the paginator to fetch all objects in the specified bucket and prefix if provided\n",
    "            files = dict()\n",
    "            for page in paginator.paginate(Bucket=bucket_name, Prefix=key):\n",
    "                # Access the 'Contents' from the page, which lists the objects\n",
    "                if \"Contents\" in page:\n",
    "                    for obj in page[\"Contents\"]:\n",
    "                        files[obj[\"Key\"]] = obj[\"Size\"]\n",
    "                        # print(f\"{obj['Key']} ({obj['Size']} bytes)\")\n",
    "            return files\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "    def download_s3_folder(self, bucket_name, local_dir, prefix=\"\", verbose=0):\n",
    "        \"\"\"\n",
    "        Download all files from an S3 bucket prefix to a local directory.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param local_dir: str. Local directory to which files will be downloaded.\n",
    "        :param prefix: str or None. Prefix path of the folder in the bucket. If None, the whole bucket is downloaded.\n",
    "        \"\"\"\n",
    "        if not prefix.endswith(\"/\"):\n",
    "            prefix = prefix + \"/\"\n",
    "            print(prefix)\n",
    "        # Ensure local directory exists\n",
    "        if prefix == \"\":\n",
    "            local_dir = os.path.join(local_dir, bucket_name)\n",
    "        else:\n",
    "            local_dir = os.path.join(local_dir, stem(prefix))\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "\n",
    "        # List objects within the specified prefix\n",
    "        paginator = self.s3_client.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n",
    "            for obj in page.get(\"Contents\", []):\n",
    "                key = obj[\"Key\"]\n",
    "                if not key.endswith(\"/\"):  # skip directories/folders\n",
    "                    # Define file path locally in same structure\n",
    "                    local_file_path = os.path.join(local_dir, key[len(prefix) :])\n",
    "                    local_file_dir = os.path.dirname(local_file_path)\n",
    "\n",
    "                    # Ensure local file directory exists\n",
    "                    if not os.path.exists(local_file_dir):\n",
    "                        os.makedirs(local_file_dir)\n",
    "\n",
    "                    # Download the file\n",
    "                    self.s3_client.download_file(bucket_name, key, local_file_path)\n",
    "                    if verbose:\n",
    "                        print(f\"Downloaded {key} to {local_file_path}\")\n",
    "\n",
    "    def download_s3_file(self, bucket_name, key, local_dir, metadata=False, verbose=0):\n",
    "        \"\"\"\n",
    "        Download a specific file from an S3 bucket and optionally return its metadata.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param key: str. The key of the file in the S3 bucket.\n",
    "        :param local_dir: str. Local directory to which the file will be downloaded.\n",
    "        :param metadata: bool. If True, return the file's metadata; otherwise, return None.\n",
    "        :param verbose: bool.\n",
    "        :return: dict or None. Returns metadata of the file if metadata is True, otherwise None.\n",
    "        \"\"\"\n",
    "        # Define the local file path\n",
    "        local_file_path = os.path.join(local_dir, os.path.basename(key))\n",
    "\n",
    "        # Ensure the local directory exists\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir)\n",
    "\n",
    "        # Download the file\n",
    "        self.s3_client.download_file(bucket_name, key, local_file_path)\n",
    "        if verbose:\n",
    "            print(f\"Downloaded {key} to {local_file_path}\")\n",
    "\n",
    "        # Optionally retrieve and return metadata\n",
    "        if metadata:\n",
    "            response = self.s3_client.head_object(Bucket=bucket_name, Key=key)\n",
    "            return response  # Return the metadata dictionary\n",
    "        return None\n",
    "\n",
    "    def upload_file_to_s3(self, bucket_name, localfile_path, s3_key, metadata=None):\n",
    "        \"\"\"\n",
    "        Upload a file to an S3 bucket with optional metadata.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param localfile_path: str. Local path to the file to be uploaded.\n",
    "        :param s3_key: str. S3 key (path within the bucket) where the file will be stored with file name included.\n",
    "        :param metadata: dict or None. Optional metadata for the file. Defaults to None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Setup the file upload options\n",
    "            extra_args = {}\n",
    "            if metadata:\n",
    "                extra_args[\"Metadata\"] = metadata\n",
    "\n",
    "            # Perform the file upload\n",
    "            with open(localfile_path, \"rb\") as file_data:\n",
    "                self.s3_client.upload_fileobj(\n",
    "                    Fileobj=file_data,\n",
    "                    Bucket=bucket_name,\n",
    "                    Key=s3_key,\n",
    "                    ExtraArgs=extra_args,\n",
    "                )\n",
    "            print(f\"File uploaded successfully to {bucket_name}/{s3_key}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload file: {e}\")\n",
    "\n",
    "    def upload_folder_to_s3(\n",
    "        self, bucket_name, local_folder_path, s3_prefix=\"\", metadata=None, verbose=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Upload all files in a local folder to an S3 bucket with optional metadata.\n",
    "\n",
    "        :param bucket_name: str. Name of the S3 bucket.\n",
    "        :param local_folder_path: str. Local path to the folder to be uploaded.\n",
    "        :param s3_prefix: str. S3 prefix (folder path within the bucket) where the files will be stored.\n",
    "                           Defaults to the root of the bucket.\n",
    "        :param metadata: dict or None. Optional metadata for the files. Defaults to None.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Ensure the local_folder_path ends with a slash to properly preserve folder structure\n",
    "            local_folder_path = os.path.normpath(local_folder_path)\n",
    "\n",
    "            # Iterate over all files in the folder and its subfolders\n",
    "            for root, dirs, files in os.walk(local_folder_path):\n",
    "                for filename in files:\n",
    "                    local_file_path = os.path.join(root, filename)\n",
    "\n",
    "                    # Generate the corresponding S3 key (prefix + relative file path)\n",
    "                    relative_path = os.path.relpath(\n",
    "                        local_file_path, os.path.dirname(local_folder_path)\n",
    "                    )\n",
    "                    s3_key = os.path.join(s3_prefix, relative_path).replace(\n",
    "                        \"\\\\\", \"/\"\n",
    "                    )  # Replace Windows path separators\n",
    "\n",
    "                    extra_args = {}\n",
    "                    if metadata:\n",
    "                        extra_args[\"Metadata\"] = metadata\n",
    "\n",
    "                    with open(local_file_path, \"rb\") as file_data:\n",
    "                        self.s3_client.upload_fileobj(\n",
    "                            Fileobj=file_data,\n",
    "                            Bucket=bucket_name,\n",
    "                            Key=s3_key,\n",
    "                            ExtraArgs=extra_args,\n",
    "                        )\n",
    "                    if verbose:\n",
    "                        print(f\"Uploaded {s3_key} to {bucket_name}/{s3_key}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upload folder: {e}\")\n",
    "\n",
    "    def inmemory_download_s3(bucket_name, key):\n",
    "        \"\"\"\n",
    "        Downloads a file from an Amazon S3 bucket and loads it directly into a pandas DataFrame.\n",
    "        The function automatically detects the file format based on its extension.\n",
    "\n",
    "        Parameters:\n",
    "        key (str): The S3 object key of the file to download.\n",
    "        bucket (str, optional): The name of the S3 bucket. Defaults to AWS_BUCKET from .env if not provided.\n",
    "        \"\"\"\n",
    "        response = self.s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        file_content = response[\"Body\"].read()\n",
    "        return file_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mys3 = S3FileHandler(aws_access_key_id, aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all Buckets\n",
    "To lists all the s3 buckets in s3 for given credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['buckettest0011',\n",
       " 'candidate-proctoring',\n",
       " 'sagemaker-ap-south-1-011528263565',\n",
       " 'sagemaker-studio-011528263565-u1h3juay9nd',\n",
       " 'sentiment-classification-fastapi']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.list_s3_buckets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List all file objects\n",
    "List all files in an S3 bucket or within a specific prefix of the given bucket along with the file size.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param key: str or None. Specific prefix to list files from, defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attendee_db/sumanth.jpg': 170670,\n",
       " 'attendee_db/test/test/line_profiling_results.txt': 921,\n",
       " 'attendee_db/test/test/outer_function_profile.txt': 2845,\n",
       " 'attendee_db/test_2.mp4': 16330195,\n",
       " 'test/test': 921,\n",
       " 'test/test/line_profiling_results.txt': 921,\n",
       " 'test/test/outer_function_profile.txt': 2845}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.list_s3_objects(bucket_name=\"buckettest0011\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 Folder Download\n",
    "Download all files from an S3 bucket prefix to a local directory.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param local_dir: str. Local directory to which files will be downloaded.  \n",
    ":param prefix: str or None. Prefix path of the folder in the bucket. If None, the whole bucket is downloaded.  \n",
    ":param verbose: bool. Display the download status  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/test/\n",
      "Downloaded test/test/line_profiling_results.txt to ./test/line_profiling_results.txt\n",
      "Downloaded test/test/outer_function_profile.txt to ./test/outer_function_profile.txt\n"
     ]
    }
   ],
   "source": [
    "mys3.download_s3_folder(\n",
    "    bucket_name=\"buckettest0011\", local_dir=\".\", prefix=\"test/test\", verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S3 File Download\n",
    "Download a specific file from an S3 bucket and optionally return its metadata.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param key: str. The key of the file in the S3 bucket.  \n",
    ":param local_dir: str. Local directory to which the file will be downloaded.  \n",
    ":param metadata: bool. If True, return the file's metadata; otherwise, return None.  \n",
    ":param verbose: bool.  \n",
    ":return: dict or None. Returns metadata of the file if metadata is True, otherwise None.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'D699DNH1XH4995EM',\n",
       "  'HostId': 'R8MIFqVr0MyVvOwbfM+ZkrgLyxHsPTp8HCqC/x0L5gR+rr9NIQZcVFwJWsmidXJe+VZRclVnONw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'R8MIFqVr0MyVvOwbfM+ZkrgLyxHsPTp8HCqC/x0L5gR+rr9NIQZcVFwJWsmidXJe+VZRclVnONw=',\n",
       "   'x-amz-request-id': 'D699DNH1XH4995EM',\n",
       "   'date': 'Wed, 16 Oct 2024 05:48:23 GMT',\n",
       "   'last-modified': 'Tue, 15 Oct 2024 09:40:40 GMT',\n",
       "   'etag': '\"7c49753bd7d2109ce96bd2568ad8fbef\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'x-amz-meta-author': 'XXXXX',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '2845'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2024, 10, 15, 9, 40, 40, tzinfo=tzutc()),\n",
       " 'ContentLength': 2845,\n",
       " 'ETag': '\"7c49753bd7d2109ce96bd2568ad8fbef\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {'author': 'XXXXX'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.download_s3_file(\n",
    "    bucket_name=\"buckettest0011\",\n",
    "    key=\"test/test/outer_function_profile.txt\",\n",
    "    local_dir=\".\",\n",
    "    metadata=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading file from local to s3 with/without metadata\n",
    "Upload a file to an S3 bucket with optional metadata.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param localfile_path: str. Local path to the file to be uploaded.  \n",
    ":param s3_key: str. S3 key (path within the bucket) where the file will be stored with file name included.  \n",
    ":param metadata: dict or None. Optional metadata for the file. Defaults to None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to buckettest0011/test/test/line_profiling_results.txt\n"
     ]
    }
   ],
   "source": [
    "mys3.upload_file_to_s3(\n",
    "    bucket_name=\"buckettest0011\",\n",
    "    localfile_path=\"/home/user/Documents/line_profiling_results.txt\",\n",
    "    s3_key=\"test/test/line_profiling_results.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded successfully to buckettest0011/test/test/line_profiling_results.txt\n"
     ]
    }
   ],
   "source": [
    "metadata = {\"author\": \"xxxxx\"}\n",
    "mys3.upload_file_to_s3(\n",
    "    bucket_name=\"buckettest0011\",\n",
    "    localfile_path=\"/home/user/Documents/line_profiling_results.txt\",\n",
    "    s3_key=\"test/test/line_profiling_results.txt\",\n",
    "    metadata=metadata,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets check by downloading the uploaded file if the metadata is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded test/test/line_profiling_results.txt to ./line_profiling_results.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': 'D69ARVG7KASXKQH1',\n",
       "  'HostId': 'Je/oIjsM1FAf2psIv4aoclG62HSr9CGpXR/zvagTThcupuCz5FsMdN7ecT243Of+/jH2mOCha30=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': 'Je/oIjsM1FAf2psIv4aoclG62HSr9CGpXR/zvagTThcupuCz5FsMdN7ecT243Of+/jH2mOCha30=',\n",
       "   'x-amz-request-id': 'D69ARVG7KASXKQH1',\n",
       "   'date': 'Wed, 16 Oct 2024 05:48:23 GMT',\n",
       "   'last-modified': 'Wed, 16 Oct 2024 05:48:23 GMT',\n",
       "   'etag': '\"5a627cd11fe9a0ec5877b4a4f0f33a62\"',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'x-amz-meta-author': 'xxxxx',\n",
       "   'accept-ranges': 'bytes',\n",
       "   'content-type': 'binary/octet-stream',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '921'},\n",
       "  'RetryAttempts': 0},\n",
       " 'AcceptRanges': 'bytes',\n",
       " 'LastModified': datetime.datetime(2024, 10, 16, 5, 48, 23, tzinfo=tzutc()),\n",
       " 'ContentLength': 921,\n",
       " 'ETag': '\"5a627cd11fe9a0ec5877b4a4f0f33a62\"',\n",
       " 'ContentType': 'binary/octet-stream',\n",
       " 'ServerSideEncryption': 'AES256',\n",
       " 'Metadata': {'author': 'xxxxx'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mys3.download_s3_file(\n",
    "    bucket_name=\"buckettest0011\",\n",
    "    key=\"test/test/line_profiling_results.txt\",\n",
    "    local_dir=\".\",\n",
    "    metadata=True,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading entire folder from local to s3 with/without metadata\n",
    "Upload all files in a local folder to an S3 bucket with optional metadata.\n",
    "\n",
    ":param bucket_name: str. Name of the S3 bucket.  \n",
    ":param local_folder_path: str. Local path to the folder to be uploaded.  \n",
    ":param s3_prefix: str. S3 prefix (folder path within the bucket) where the files will be stored.  \n",
    "                    Defaults to the root of the bucket.  \n",
    ":param metadata: dict or None. Optional metadata for the files. Defaults to None.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded attendee_db/sumanth.jpg to buckettest0011/attendee_db/sumanth.jpg\n",
      "Uploaded attendee_db/test_2.mp4 to buckettest0011/attendee_db/test_2.mp4\n",
      "Uploaded attendee_db/test/test/outer_function_profile.txt to buckettest0011/attendee_db/test/test/outer_function_profile.txt\n",
      "Uploaded attendee_db/test/test/line_profiling_results.txt to buckettest0011/attendee_db/test/test/line_profiling_results.txt\n"
     ]
    }
   ],
   "source": [
    "mys3.upload_folder_to_s3(\n",
    "    \"buckettest0011\", \"/home/user/Documents/attendee_db\", verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luminaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
